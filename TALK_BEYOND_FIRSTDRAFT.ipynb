{"cells":[{"cell_type":"markdown","metadata":{"id":"Tb9E-dFaGqGr"},"source":["# Introduction\n","\n","We will be using Colab as IDE for our project. Through colab we will be able to diectly access Google drive where our data is stored and have the advantage of GPUs which will help us with computational power."]},{"cell_type":"markdown","metadata":{"id":"R356hX8OHKmj"},"source":["First we will be mounting our drive to this notebook"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16957,"status":"ok","timestamp":1702263671254,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"EU9XbSufw8cu","outputId":"7a1b3852-ae34-4b94-f25c-895a685d8b8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"KSVFgMPsIaIi"},"source":["#Install packages\n","Colab does not provide us with all the modules/packlages. Whenever we are using modules other than the built-in, we should install the seperately as shown below."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7932,"status":"ok","timestamp":1702263679183,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"HSeH0N5lpMUA","outputId":"def62e01-bbe7-416a-9205-4cf20e2c6ea0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/981.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.6/981.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=d58d7fd834bac5b8d7a7f0cf9c83ff103e96e3abb28b252ccf2318840e027a4d\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n"]}],"source":["!pip install langdetect"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5733,"status":"ok","timestamp":1702263684904,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"0jA4OUnwxKAU","outputId":"0330f099-7bdd-45ba-bef6-5da87dd2510a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unidecode\n","  Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m204.8/235.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.7\n"]}],"source":["!pip install unidecode"]},{"cell_type":"markdown","metadata":{"id":"43G2_QMsIgbC"},"source":["#Import Modules\n","We should import all the packages that we use in this project"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29919,"status":"ok","timestamp":1702263714821,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"Dt75mtY2xKJ-","outputId":"40080c53-0ad4-442c-d49a-fe49780c2c09"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pe08.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]}],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('all')\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import random\n","from nltk import tokenize\n","import string\n","from unidecode import unidecode\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding, LayerNormalization, MultiHeadAttention, Add, Bidirectional, Concatenate, Bidirectional, Attention\n","from keras.models import Model\n","from langdetect import detect"]},{"cell_type":"markdown","metadata":{"id":"nWyUreLOI0Zw"},"source":["#Loading Data\n","Once the drive was connected, we have to load the data and rename the columns if required. Then deal with negatives if there any. Since this is a english to french data set, it is less likely to have missing values but we should focus on data cleasing for sure."]},{"cell_type":"code","execution_count":112,"metadata":{"id":"WJp8lQx1xKFS","executionInfo":{"status":"ok","timestamp":1702268544602,"user_tz":300,"elapsed":1360,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"outputs":[],"source":["# load the data\n","data = pd.read_csv(\"/content/drive/MyDrive/eng_-french.csv\")\n","data = data.rename(columns={'en': 'english', 'fr': 'french'})\n","\n","data = data.dropna()\n","data.columns = ['en', 'fr']"]},{"cell_type":"markdown","metadata":{"id":"4EFwexeGJwyM"},"source":["Let's check the schema of the data."]},{"cell_type":"code","execution_count":113,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1702268545169,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"VhGDZzhwxKCr","outputId":"bab94a62-af6c-4e54-8b56-533f839770d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 175621 entries, 0 to 175620\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count   Dtype \n","---  ------  --------------   ----- \n"," 0   en      175621 non-null  object\n"," 1   fr      175621 non-null  object\n","dtypes: object(2)\n","memory usage: 2.7+ MB\n"]}],"source":["data.info()"]},{"cell_type":"markdown","metadata":{"id":"id9FZTRJKSra"},"source":["Lets count the number of sentences in each text entry of the 'en' column of the DataFrame data, and then print a statistical summary of these counts. This can be useful for understanding the distribution of sentence lengths in the dataset, which is often an important aspect in text analysis and natural language processing."]},{"cell_type":"code","execution_count":114,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2353,"status":"ok","timestamp":1702268547516,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"yaJuk4NaxJ7X","outputId":"1821a4c1-59e7-4674-ef88-e69f9714a18e"},"outputs":[{"output_type":"stream","name":"stdout","text":["count    175621.000000\n","mean          1.009281\n","std           0.098643\n","min           1.000000\n","25%           1.000000\n","50%           1.000000\n","75%           1.000000\n","max           4.000000\n","Name: en, dtype: float64\n"]}],"source":["# Assuming 'data' is your DataFrame and 'column_name' is the name of the column you're interested in\n","sentence_counts = data['en'].map(lambda x: len(tokenize.sent_tokenize(x)))\n","print(sentence_counts.describe())"]},{"cell_type":"code","execution_count":115,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2486,"status":"ok","timestamp":1702268549997,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"FdOPuKyFxJ4Q","outputId":"eae7a943-2f84-4689-e39f-be123d59a3af"},"outputs":[{"output_type":"stream","name":"stdout","text":["count    175621.000000\n","mean          1.010039\n","std           0.113824\n","min           1.000000\n","25%           1.000000\n","50%           1.000000\n","75%           1.000000\n","max           5.000000\n","Name: fr, dtype: float64\n"]}],"source":["sentence_counts = data['fr'].map(lambda x: len(tokenize.sent_tokenize(x)))\n","print(sentence_counts.describe())"]},{"cell_type":"code","execution_count":116,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1702268549997,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"9mW0T-XExJ16","outputId":"39aa6cfe-cb6b-4fd7-e2b5-516af77ec2c9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\n',\n"," ' ',\n"," '!',\n"," '\"',\n"," '$',\n"," '%',\n"," '&',\n"," \"'\",\n"," '(',\n"," ')',\n"," '+',\n"," ',',\n"," '-',\n"," '.',\n"," '/',\n"," '0',\n"," '1',\n"," '2',\n"," '3',\n"," '4',\n"," '5',\n"," '6',\n"," '7',\n"," '8',\n"," '9',\n"," ':',\n"," ';',\n"," '?',\n"," 'A',\n"," 'B',\n"," 'C',\n"," 'D',\n"," 'E',\n"," 'F',\n"," 'G',\n"," 'H',\n"," 'I',\n"," 'J',\n"," 'K',\n"," 'L',\n"," 'M',\n"," 'N',\n"," 'O',\n"," 'P',\n"," 'Q',\n"," 'R',\n"," 'S',\n"," 'T',\n"," 'U',\n"," 'V',\n"," 'W',\n"," 'X',\n"," 'Y',\n"," 'Z',\n"," 'a',\n"," 'b',\n"," 'c',\n"," 'd',\n"," 'e',\n"," 'f',\n"," 'g',\n"," 'h',\n"," 'i',\n"," 'j',\n"," 'k',\n"," 'l',\n"," 'm',\n"," 'n',\n"," 'o',\n"," 'p',\n"," 'q',\n"," 'r',\n"," 's',\n"," 't',\n"," 'u',\n"," 'v',\n"," 'w',\n"," 'x',\n"," 'y',\n"," 'z',\n"," '\\xa0',\n"," '«',\n"," '»',\n"," 'À',\n"," 'Â',\n"," 'Ç',\n"," 'É',\n"," 'Ê',\n"," 'Ô',\n"," 'à',\n"," 'á',\n"," 'â',\n"," 'ç',\n"," 'è',\n"," 'é',\n"," 'ê',\n"," 'ë',\n"," 'î',\n"," 'ï',\n"," 'ô',\n"," 'ö',\n"," 'ù',\n"," 'û',\n"," 'œ',\n"," 'С',\n"," '\\u2009',\n"," '\\u200b',\n"," '–',\n"," '‘',\n"," '’',\n"," '…',\n"," '\\u202f',\n"," '‽',\n"," '₂']"]},"metadata":{},"execution_count":116}],"source":["data_fr = '\\n'.join(data['fr'].tolist())\n","chars_fr = sorted(list(set(data_fr)))\n","chars_fr"]},{"cell_type":"code","execution_count":117,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1702268549997,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"TzF0R3OtxJzD","outputId":"7e15d2b4-f5a5-4801-9ef0-9a920bde107c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\n',\n"," ' ',\n"," '!',\n"," '\"',\n"," '$',\n"," '%',\n"," '&',\n"," \"'\",\n"," '+',\n"," ',',\n"," '-',\n"," '.',\n"," '/',\n"," '0',\n"," '1',\n"," '2',\n"," '3',\n"," '4',\n"," '5',\n"," '6',\n"," '7',\n"," '8',\n"," '9',\n"," ':',\n"," ';',\n"," '?',\n"," 'A',\n"," 'B',\n"," 'C',\n"," 'D',\n"," 'E',\n"," 'F',\n"," 'G',\n"," 'H',\n"," 'I',\n"," 'J',\n"," 'K',\n"," 'L',\n"," 'M',\n"," 'N',\n"," 'O',\n"," 'P',\n"," 'Q',\n"," 'R',\n"," 'S',\n"," 'T',\n"," 'U',\n"," 'V',\n"," 'W',\n"," 'X',\n"," 'Y',\n"," 'Z',\n"," 'a',\n"," 'b',\n"," 'c',\n"," 'd',\n"," 'e',\n"," 'f',\n"," 'g',\n"," 'h',\n"," 'i',\n"," 'j',\n"," 'k',\n"," 'l',\n"," 'm',\n"," 'n',\n"," 'o',\n"," 'p',\n"," 'q',\n"," 'r',\n"," 's',\n"," 't',\n"," 'u',\n"," 'v',\n"," 'w',\n"," 'x',\n"," 'y',\n"," 'z',\n"," '\\xa0',\n"," '\\xad',\n"," 'º',\n"," 'ç',\n"," 'é',\n"," 'ö',\n"," 'ú',\n"," 'а',\n"," '–',\n"," '—',\n"," '‘',\n"," '’',\n"," '₂',\n"," '€']"]},"metadata":{},"execution_count":117}],"source":["data_en = '\\n'.join(data['en'].tolist())\n","chars_en = sorted(list(set(data_en)))\n","chars_en"]},{"cell_type":"code","execution_count":118,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5256,"status":"ok","timestamp":1702268555251,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"GPrzDvhLxJtU","outputId":"f6bc15de-6fea-440f-cb02-16d1c7c9456e"},"outputs":[{"output_type":"stream","name":"stdout","text":["                                                 en_clean  \\\n","0                                       <START> hi. <END>   \n","1                                      <START> run! <END>   \n","2                                      <START> run! <END>   \n","3                                      <START> who? <END>   \n","4                                      <START> wow! <END>   \n","...                                                   ...   \n","175616  <START> top-down economics never works, said o...   \n","175617  <START> a carbon footprint is the amount of ca...   \n","175618  <START> death is something that we're often di...   \n","175619  <START> since there are usually multiple websi...   \n","175620  <START> if someone who doesn't know your backg...   \n","\n","                                                 fr_clean  \n","0                                    <START> salut! <END>  \n","1                                   <START> cours ! <END>  \n","2                                  <START> courez ! <END>  \n","3                                     <START> qui ? <END>  \n","4                                <START> ca alors ! <END>  \n","...                                                   ...  \n","175616  <START> << l'economie en partant du haut vers ...  \n","175617  <START> une empreinte carbone est la somme de ...  \n","175618  <START> la mort est une chose qu'on nous decou...  \n","175619  <START> puisqu'il y a de multiples sites web s...  \n","175620  <START> si quelqu'un qui ne connait pas vos an...  \n","\n","[175621 rows x 2 columns]\n"]}],"source":["\n","# Sample DataFrame creation for demonstration (you'll use your own DataFrame)\n","# data = pd.DataFrame({\n","#     'english': ['This is a sample text.', 'Another example text.'],\n","#     'french': ['Ceci est un texte d\\'exemple.', 'Un autre texte d\\'exemple.']\n","# })\n","\n","# Constants\n","start_token = '<START>'\n","end_token = '<END>'\n","unk_token = '<UNK>'\n","pad_token = '<PAD>'\n","\n","def clean_text(text, start_token=start_token, end_token=end_token):\n","    text = text.lower()  # convert uppercase to lowercase\n","    text = unidecode(text, errors='ignore')  # convert accented letters into unaccented letters. Ignore unknown characters.\n","    text = ''.join((char if char in (string.punctuation + string.ascii_lowercase + ' ') else ' ' for char in text))  # keep the selected letters and punctuation.\n","\n","    # Add start and end tokens\n","    text = f\"{start_token} {text} {end_token}\"\n","    return text\n","\n","# Apply the cleaning function to each element in the desired columns\n","data['en_clean'] = data['en'].apply(clean_text)\n","data['fr_clean'] = data['fr'].apply(clean_text)\n","\n","# Display the cleaned data\n","print(data[['en_clean', 'fr_clean']])\n"]},{"cell_type":"code","execution_count":119,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1702268555251,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"VJmyN-jKxJqs","outputId":"b0aaf669-d73d-4722-aa13-9bcc167429ea"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\n',\n"," ' ',\n"," '!',\n"," '\"',\n"," '$',\n"," '%',\n"," '&',\n"," \"'\",\n"," '+',\n"," ',',\n"," '-',\n"," '.',\n"," '/',\n"," ':',\n"," ';',\n"," '<',\n"," '>',\n"," '?',\n"," 'A',\n"," 'D',\n"," 'E',\n"," 'N',\n"," 'R',\n"," 'S',\n"," 'T',\n"," 'a',\n"," 'b',\n"," 'c',\n"," 'd',\n"," 'e',\n"," 'f',\n"," 'g',\n"," 'h',\n"," 'i',\n"," 'j',\n"," 'k',\n"," 'l',\n"," 'm',\n"," 'n',\n"," 'o',\n"," 'p',\n"," 'q',\n"," 'r',\n"," 's',\n"," 't',\n"," 'u',\n"," 'v',\n"," 'w',\n"," 'x',\n"," 'y',\n"," 'z']"]},"metadata":{},"execution_count":119}],"source":["data_en = '\\n'.join(data['en_clean'].tolist())\n","chars_en = sorted(list(set(data_en)))\n","chars_en"]},{"cell_type":"code","execution_count":120,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1702268555252,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"B56NaP01xJn1","outputId":"91649cba-5af7-429a-c4d0-711cd84ae4a9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\n',\n"," ' ',\n"," '!',\n"," '\"',\n"," '$',\n"," '%',\n"," '&',\n"," \"'\",\n"," '(',\n"," ')',\n"," '+',\n"," ',',\n"," '-',\n"," '.',\n"," '/',\n"," ':',\n"," ';',\n"," '<',\n"," '>',\n"," '?',\n"," 'A',\n"," 'D',\n"," 'E',\n"," 'N',\n"," 'R',\n"," 'S',\n"," 'T',\n"," 'a',\n"," 'b',\n"," 'c',\n"," 'd',\n"," 'e',\n"," 'f',\n"," 'g',\n"," 'h',\n"," 'i',\n"," 'j',\n"," 'k',\n"," 'l',\n"," 'm',\n"," 'n',\n"," 'o',\n"," 'p',\n"," 'q',\n"," 'r',\n"," 's',\n"," 't',\n"," 'u',\n"," 'v',\n"," 'w',\n"," 'x',\n"," 'y',\n"," 'z']"]},"metadata":{},"execution_count":120}],"source":["data_fr = '\\n'.join(data['fr_clean'].tolist())\n","chars_fr = sorted(list(set(data_fr)))\n","chars_fr"]},{"cell_type":"code","execution_count":121,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":493},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1702268555252,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"__OND1eJxJle","outputId":"059a11b9-c978-4fed-b748-b3d5dd3aaca3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                       en  \\\n","0                                                     Hi.   \n","1                                                    Run!   \n","2                                                    Run!   \n","3                                                    Who?   \n","4                                                    Wow!   \n","...                                                   ...   \n","175616  Top-down economics never works, said Obama. \"T...   \n","175617  A carbon footprint is the amount of carbon dio...   \n","175618  Death is something that we're often discourage...   \n","175619  Since there are usually multiple websites on a...   \n","175620  If someone who doesn't know your background sa...   \n","\n","                                                       fr  \\\n","0                                                  Salut!   \n","1                                                 Cours !   \n","2                                                Courez !   \n","3                                                   Qui ?   \n","4                                              Ça alors !   \n","...                                                   ...   \n","175616  « L'économie en partant du haut vers le bas, ç...   \n","175617  Une empreinte carbone est la somme de pollutio...   \n","175618  La mort est une chose qu'on nous décourage sou...   \n","175619  Puisqu'il y a de multiples sites web sur chaqu...   \n","175620  Si quelqu'un qui ne connaît pas vos antécédent...   \n","\n","                                                 en_clean  \\\n","0                                       <START> hi. <END>   \n","1                                      <START> run! <END>   \n","2                                      <START> run! <END>   \n","3                                      <START> who? <END>   \n","4                                      <START> wow! <END>   \n","...                                                   ...   \n","175616  <START> top-down economics never works, said o...   \n","175617  <START> a carbon footprint is the amount of ca...   \n","175618  <START> death is something that we're often di...   \n","175619  <START> since there are usually multiple websi...   \n","175620  <START> if someone who doesn't know your backg...   \n","\n","                                                 fr_clean  \n","0                                    <START> salut! <END>  \n","1                                   <START> cours ! <END>  \n","2                                  <START> courez ! <END>  \n","3                                     <START> qui ? <END>  \n","4                                <START> ca alors ! <END>  \n","...                                                   ...  \n","175616  <START> << l'economie en partant du haut vers ...  \n","175617  <START> une empreinte carbone est la somme de ...  \n","175618  <START> la mort est une chose qu'on nous decou...  \n","175619  <START> puisqu'il y a de multiples sites web s...  \n","175620  <START> si quelqu'un qui ne connait pas vos an...  \n","\n","[175621 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-6d8be390-6468-4aa1-bb2b-10efbb7f252a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>en</th>\n","      <th>fr</th>\n","      <th>en_clean</th>\n","      <th>fr_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Hi.</td>\n","      <td>Salut!</td>\n","      <td>&lt;START&gt; hi. &lt;END&gt;</td>\n","      <td>&lt;START&gt; salut! &lt;END&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Run!</td>\n","      <td>Cours !</td>\n","      <td>&lt;START&gt; run! &lt;END&gt;</td>\n","      <td>&lt;START&gt; cours ! &lt;END&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Run!</td>\n","      <td>Courez !</td>\n","      <td>&lt;START&gt; run! &lt;END&gt;</td>\n","      <td>&lt;START&gt; courez ! &lt;END&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Who?</td>\n","      <td>Qui ?</td>\n","      <td>&lt;START&gt; who? &lt;END&gt;</td>\n","      <td>&lt;START&gt; qui ? &lt;END&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Wow!</td>\n","      <td>Ça alors !</td>\n","      <td>&lt;START&gt; wow! &lt;END&gt;</td>\n","      <td>&lt;START&gt; ca alors ! &lt;END&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>175616</th>\n","      <td>Top-down economics never works, said Obama. \"T...</td>\n","      <td>« L'économie en partant du haut vers le bas, ç...</td>\n","      <td>&lt;START&gt; top-down economics never works, said o...</td>\n","      <td>&lt;START&gt; &lt;&lt; l'economie en partant du haut vers ...</td>\n","    </tr>\n","    <tr>\n","      <th>175617</th>\n","      <td>A carbon footprint is the amount of carbon dio...</td>\n","      <td>Une empreinte carbone est la somme de pollutio...</td>\n","      <td>&lt;START&gt; a carbon footprint is the amount of ca...</td>\n","      <td>&lt;START&gt; une empreinte carbone est la somme de ...</td>\n","    </tr>\n","    <tr>\n","      <th>175618</th>\n","      <td>Death is something that we're often discourage...</td>\n","      <td>La mort est une chose qu'on nous décourage sou...</td>\n","      <td>&lt;START&gt; death is something that we're often di...</td>\n","      <td>&lt;START&gt; la mort est une chose qu'on nous decou...</td>\n","    </tr>\n","    <tr>\n","      <th>175619</th>\n","      <td>Since there are usually multiple websites on a...</td>\n","      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n","      <td>&lt;START&gt; since there are usually multiple websi...</td>\n","      <td>&lt;START&gt; puisqu'il y a de multiples sites web s...</td>\n","    </tr>\n","    <tr>\n","      <th>175620</th>\n","      <td>If someone who doesn't know your background sa...</td>\n","      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n","      <td>&lt;START&gt; if someone who doesn't know your backg...</td>\n","      <td>&lt;START&gt; si quelqu'un qui ne connait pas vos an...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>175621 rows × 4 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d8be390-6468-4aa1-bb2b-10efbb7f252a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-6d8be390-6468-4aa1-bb2b-10efbb7f252a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-6d8be390-6468-4aa1-bb2b-10efbb7f252a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-39d25e18-440c-49e8-a5a5-9224f8107ae1\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-39d25e18-440c-49e8-a5a5-9224f8107ae1')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-39d25e18-440c-49e8-a5a5-9224f8107ae1 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_3b6f8c42-40cd-43ac-b553-8003041ee5e7\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_3b6f8c42-40cd-43ac-b553-8003041ee5e7 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('data');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":121}],"source":["data"]},{"cell_type":"code","execution_count":122,"metadata":{"id":"wk8Y6qF_xJip","executionInfo":{"status":"ok","timestamp":1702268555252,"user_tz":300,"elapsed":9,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"outputs":[],"source":["sent_clean_fr = data['fr_clean'].tolist()\n","sent_clean_en = data['en_clean'].tolist()"]},{"cell_type":"code","execution_count":123,"metadata":{"id":"YXwQwx8nxJfg","executionInfo":{"status":"ok","timestamp":1702268555252,"user_tz":300,"elapsed":8,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"outputs":[],"source":["from collections import Counter"]},{"cell_type":"code","execution_count":124,"metadata":{"id":"-hwIDoKBxJdb","executionInfo":{"status":"ok","timestamp":1702268555252,"user_tz":300,"elapsed":8,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"outputs":[],"source":["class Tokenize():\n","    def __init__(self, word_vocab=None, max_tokens=None, start_token='<START>', end_token='<END>', unk_token='<UNK>', pad_token='<PAD>'):\n","        self.max_tokens = max_tokens\n","        self.start_token = start_token\n","        self.end_token = end_token\n","        self.unk_token = unk_token\n","        self.pad_token = pad_token\n","        self.word_vocab = word_vocab\n","        if self.word_vocab:\n","            self.word_vocab = [self.pad_token, self.start_token, self.end_token, self.unk_token] + [word for word,count in word_count.most_common(max_tokens)]\n","            self.word_to_idx = {w:i for i,w in enumerate(self.word_vocab)}\n","            self.idx_to_word = {i:w for i,w in enumerate(self.word_vocab)}\n","            self.len_vocab = len(self.word_vocab)\n","\n","    def fit(self, texts):\n","\n","        if type(texts) == str: texts = [texts]\n","\n","        word_count = Counter(tokenize.word_tokenize(' '.join(texts)))\n","\n","        if self.max_tokens:\n","            max_tokens = self.max_tokens -  3\n","        else:\n","            max_tokens = self.max_tokens\n","\n","        self.word_vocab = [self.pad_token, self.start_token, self.end_token, self.unk_token] + [word for word,count in word_count.most_common(max_tokens)]\n","        self.word_to_idx = {w:i for i,w in enumerate(self.word_vocab)}\n","        self.idx_to_word = {i:w for i,w in enumerate(self.word_vocab)}\n","        self.len_vocab = len(self.word_vocab)\n","\n","    def __call__(self, texts):\n","        if type(texts) == str: texts = [texts]\n","        texts_token = [[self.start_token] + tokenize.word_tokenize(text) + [self.end_token] for text in texts]\n","        texts_token_id = [[self.word_to_idx.get(word, self.word_to_idx[self.unk_token]) for word in sent] for sent in texts_token]\n","\n","        return texts_token, texts_token_id\n"]},{"cell_type":"code","execution_count":125,"metadata":{"id":"sjnUCuh8yIyy","executionInfo":{"status":"ok","timestamp":1702268555507,"user_tz":300,"elapsed":2,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"outputs":[],"source":["\n","\n","sent_clean_fr_train, sent_clean_fr_valid, sent_clean_en_train, sent_clean_en_valid = train_test_split(sent_clean_fr, sent_clean_en, test_size=0.2, random_state=0)"]},{"cell_type":"code","execution_count":126,"metadata":{"id":"kpnQj08HyIwc","executionInfo":{"status":"ok","timestamp":1702268600996,"user_tz":300,"elapsed":45491,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"outputs":[],"source":["tokenize_fr = Tokenize()\n","tokenize_fr.fit(sent_clean_fr_train)\n","train_token_fr, train_id_fr = tokenize_fr(sent_clean_fr_train)\n","valid_token_fr, valid_id_fr = tokenize_fr(sent_clean_fr_valid)"]},{"cell_type":"code","execution_count":127,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44345,"status":"ok","timestamp":1702268645330,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"GjsQoxX0yIul","outputId":"6ed0a22f-1cbd-4f89-9b01-4bf02b745d27"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["60"]},"metadata":{},"execution_count":127}],"source":["tokenize_en = Tokenize()\n","tokenize_en.fit(sent_clean_en_train)\n","train_token_en, train_id_en = tokenize_en(sent_clean_en_train)\n","valid_token_en, valid_id_en = tokenize_en(sent_clean_en_valid)\n","max_len_fr = max((len(text) for text in valid_token_fr))\n","max_len_fr"]},{"cell_type":"code","execution_count":128,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1702268645330,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"a3iFI8SgyIr9","outputId":"cda9a9f0-2162-4e35-f829-c9e875166a08"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["54"]},"metadata":{},"execution_count":128}],"source":["max_len_en = max((len(text) for text in valid_token_en))\n","max_len_en"]},{"cell_type":"code","execution_count":129,"metadata":{"id":"8B3T7vARyIpl","executionInfo":{"status":"ok","timestamp":1702268645330,"user_tz":300,"elapsed":12,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"outputs":[],"source":["max_len = 64"]},{"cell_type":"code","execution_count":130,"metadata":{"id":"Y4bX9bzfyImz","executionInfo":{"status":"ok","timestamp":1702268651289,"user_tz":300,"elapsed":5970,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"outputs":[],"source":["def make_dataset(query, value, max_length=50, return_shift=False, pad_token_id=0):\n","        if len(query) > max_length:\n","            query_pad = query[:max_length]\n","        else:\n","            query_pad = query + [pad_token_id]*(max_length-len(query))\n","\n","        if len(value) > max_length:\n","            value_pad = value[:max_length]\n","            value_shifted_pad = value[1:max_length+1]\n","        else:\n","            value_pad = value[:-1] + [pad_token_id]*(max_length-len(value[:-1]))\n","            value_shifted_pad = value[1:] + [pad_token_id]*(max_length-len(value[1:]))\n","\n","\n","        return query_pad, value_pad, value_shifted_pad\n","def shift(sent, pad_token_id=0):\n","    pad_token_shifted = sent[1:] + [pad_token_id]\n","    return pad_token_shifted\n","query_pad_train, value_pad_train, value_shifted_pad_train = map(np.array,zip(*map(make_dataset, train_id_fr, train_id_en, [max_len]*len(train_id_en))))\n","query_pad_valid, value_pad_valid, value_shifted_pad_valid = map(np.array,zip(*map(make_dataset, valid_id_fr, valid_id_en, [max_len]*len(train_id_en))))"]},{"cell_type":"code","execution_count":131,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1702268651289,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"8N8VfQ6fyIkX","outputId":"49b8b214-82f1-44a7-aeef-8618afb1af39"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  1,   4,   6,   5,  17,  15,  40, 320,  13,   4,   7,   5,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])"]},"metadata":{},"execution_count":131}],"source":["value_pad_train[0]"]},{"cell_type":"code","execution_count":132,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1702268651289,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"1IZiP1YfyIhU","outputId":"09e367e6-039c-498e-c22c-d98d777a7e51"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  4,   6,   5,  17,  15,  40, 320,  13,   4,   7,   5,   2,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])"]},"metadata":{},"execution_count":132}],"source":["value_shifted_pad_train[0]"]},{"cell_type":"code","execution_count":133,"metadata":{"id":"sSmQ8YEuyIet","executionInfo":{"status":"ok","timestamp":1702268651289,"user_tz":300,"elapsed":5,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"outputs":[],"source":["fr_vocab = tokenize_fr.len_vocab\n","en_vocab = tokenize_en.len_vocab"]},{"cell_type":"code","execution_count":134,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1702268651290,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"efj81FbSyIbn","outputId":"abcb0d7f-f9c5-45d4-eaa0-cabc0cfe0376"},"outputs":[{"output_type":"stream","name":"stdout","text":["25768 13292\n"]}],"source":["print(fr_vocab, en_vocab)"]},{"cell_type":"code","execution_count":138,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3198,"status":"ok","timestamp":1702268822569,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"ioyd4busyIZC","outputId":"7bf4613e-26da-404f-a169-6a6e5793723e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_12\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_25 (InputLayer)       [(None, 64)]                 0         []                            \n","                                                                                                  \n"," input_26 (InputLayer)       [(None, 64)]                 0         []                            \n","                                                                                                  \n"," embedding_24 (Embedding)    (None, 64, 1024)             1361100   ['input_25[0][0]']            \n","                                                          8                                       \n","                                                                                                  \n"," embedding_25 (Embedding)    (None, 64, 1024)             2638643   ['input_26[0][0]']            \n","                                                          2                                       \n","                                                                                                  \n"," lstm_24 (LSTM)              [(None, 64, 1024),           8392704   ['embedding_24[0][0]']        \n","                              (None, 1024),                                                       \n","                              (None, 1024)]                                                       \n","                                                                                                  \n"," lstm_25 (LSTM)              (None, 64, 1024)             8392704   ['embedding_25[0][0]',        \n","                                                                     'lstm_24[0][1]',             \n","                                                                     'lstm_24[0][2]']             \n","                                                                                                  \n"," dense_12 (Dense)            (None, 64, 25768)            2641220   ['lstm_25[0][0]']             \n","                                                          0                                       \n","                                                                                                  \n","==================================================================================================\n","Total params: 83195048 (317.36 MB)\n","Trainable params: 83195048 (317.36 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["\n","UNITS = 1024\n","EPOCHS = 10\n","BATCH_SIZE = 256\n","\n","# Define an input sequence and process it.\n","encoder_inputs = Input(shape=(max_len))\n","x = Embedding(en_vocab, UNITS)(encoder_inputs)\n","encoder_outputs, state_h, state_c = LSTM(UNITS, return_sequences=True, return_state=True, dropout=0.2)(x)\n","encoder_states = [state_h, state_c]\n","\n","\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = Input(shape=(max_len))\n","decoder_embeddings = Embedding(fr_vocab, UNITS)(decoder_inputs)\n","\n","\n","\n","x = LSTM(UNITS, return_sequences=True, dropout=0.2)(decoder_embeddings, initial_state=encoder_states)\n","\n","decoder_outputs = Dense(fr_vocab)(x)\n","\n","\n","\n","en_fr_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","en_fr_model.summary()\n"]},{"cell_type":"code","execution_count":139,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3083,"status":"ok","timestamp":1702268825647,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"},"user_tz":300},"id":"EpEJn4jTK31P","outputId":"5debd866-8ad6-4749-83e0-38f46d6e8bfd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_13\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_27 (InputLayer)       [(None, 64)]                 0         []                            \n","                                                                                                  \n"," input_28 (InputLayer)       [(None, 64)]                 0         []                            \n","                                                                                                  \n"," embedding_26 (Embedding)    (None, 64, 1024)             2638643   ['input_27[0][0]']            \n","                                                          2                                       \n","                                                                                                  \n"," embedding_27 (Embedding)    (None, 64, 1024)             1361100   ['input_28[0][0]']            \n","                                                          8                                       \n","                                                                                                  \n"," lstm_26 (LSTM)              [(None, 64, 1024),           8392704   ['embedding_26[0][0]']        \n","                              (None, 1024),                                                       \n","                              (None, 1024)]                                                       \n","                                                                                                  \n"," lstm_27 (LSTM)              (None, 64, 1024)             8392704   ['embedding_27[0][0]',        \n","                                                                     'lstm_26[0][1]',             \n","                                                                     'lstm_26[0][2]']             \n","                                                                                                  \n"," dense_13 (Dense)            (None, 64, 13292)            1362430   ['lstm_27[0][0]']             \n","                                                          0                                       \n","                                                                                                  \n","==================================================================================================\n","Total params: 70407148 (268.58 MB)\n","Trainable params: 70407148 (268.58 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["# Define an input sequence and process it.\n","encoder_inputs1 = Input(shape=(max_len))\n","y = Embedding(fr_vocab, UNITS)(encoder_inputs1)\n","encoder_outputs1, state_h1, state_c1 = LSTM(UNITS, return_sequences=True, return_state=True, dropout=0.2)(y)\n","encoder_states1 = [state_h1, state_c1]\n","\n","\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs1 = Input(shape=(max_len))\n","decoder_embeddings1 = Embedding(en_vocab, UNITS)(decoder_inputs1)\n","\n","\n","\n","y = LSTM(UNITS, return_sequences=True, dropout=0.2)(decoder_embeddings1, initial_state=encoder_states1)\n","\n","decoder_outputs1 = Dense(en_vocab)(y)\n","\n","\n","\n","fr_en_model = Model([encoder_inputs1, decoder_inputs1], decoder_outputs1)\n","fr_en_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CGtq0klxyIWb","outputId":"b824538b-9d69-4f5a-ca9c-ff94021d9718"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","549/549 [==============================] - 252s 451ms/step - loss: 2.9248 - masked_acc: 0.5535 - masked_loss: 2.9246 - val_loss: 2.3912 - val_masked_acc: 0.6097 - val_masked_loss: 2.3914\n","Epoch 2/10\n","549/549 [==============================] - 248s 452ms/step - loss: 2.2092 - masked_acc: 0.6247 - masked_loss: 2.2091 - val_loss: 2.0809 - val_masked_acc: 0.6334 - val_masked_loss: 2.0811\n","Epoch 3/10\n","549/549 [==============================] - 249s 454ms/step - loss: 1.9668 - masked_acc: 0.6424 - masked_loss: 1.9668 - val_loss: 1.9342 - val_masked_acc: 0.6460 - val_masked_loss: 1.9344\n","Epoch 4/10\n","318/549 [================>.............] - ETA: 1:34 - loss: 1.8390 - masked_acc: 0.6521 - masked_loss: 1.8390"]}],"source":["def masked_loss(y_true, y_pred):\n","    # Calculate the loss for each item in the batch.\n","    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True, reduction='none')\n","    loss = loss_fn(y_true, y_pred)\n","\n","    # Mask off the losses on padding.\n","    mask = tf.cast(y_true != 0, loss.dtype)\n","    loss *= mask\n","\n","    # Return the total.\n","    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n","\n","def masked_acc(y_true, y_pred):\n","    # Calculate the accuracy for each item in the batch.\n","    y_pred = tf.argmax(y_pred, axis=-1)\n","    y_pred = tf.cast(y_pred, y_true.dtype)\n","\n","    match = tf.cast(y_true == y_pred, tf.float32)\n","    mask = tf.cast(y_true != 0, tf.float32)\n","\n","    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n","# Compile & run training\n","en_fr_model.compile(optimizer='adam', loss=masked_loss, metrics=[masked_acc, masked_loss])\n","history = en_fr_model.fit([query_pad_train, value_pad_train], value_shifted_pad_train,\n","          batch_size=BATCH_SIZE,\n","          epochs=EPOCHS,\n","          validation_data=([query_pad_valid, value_pad_valid], value_shifted_pad_valid)\n","                   )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EQeMLJnvyITq"},"outputs":[],"source":["plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'], label='loss')\n","plt.plot(history.history['val_loss'], label='val_loss')\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch #')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['masked_acc'], label='accuracy')\n","plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch #')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","en_fr_history = history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tgy0GQpVL3O6","executionInfo":{"status":"aborted","timestamp":1702264043410,"user_tz":300,"elapsed":10,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"outputs":[],"source":["# Save the model in the native Keras format\n","en_fr_model.save('en_fr_translation_model.keras')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UzthoNWLuZ0S","executionInfo":{"status":"aborted","timestamp":1702264043410,"user_tz":300,"elapsed":10,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"outputs":[],"source":["def masked_loss(y_true, y_pred):\n","    # Calculate the loss for each item in the batch.\n","    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True, reduction='none')\n","    loss = loss_fn(y_true, y_pred)\n","\n","    # Mask off the losses on padding.\n","    mask = tf.cast(y_true != 0, loss.dtype)\n","    loss *= mask\n","\n","    # Return the total.\n","    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n","\n","def masked_acc(y_true, y_pred):\n","    # Calculate the accuracy for each item in the batch.\n","    y_pred = tf.argmax(y_pred, axis=-1)\n","    y_pred = tf.cast(y_pred, y_true.dtype)\n","\n","    match = tf.cast(y_true == y_pred, tf.float32)\n","    mask = tf.cast(y_true != 0, tf.float32)\n","\n","    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n","# Compile & run training\n","fr_en_model.compile(optimizer='adam', loss=masked_loss, metrics=[masked_acc, masked_loss])\n","history = fr_en_model.fit([query_pad_train, value_pad_train], value_shifted_pad_train,\n","          batch_size=BATCH_SIZE,\n","          epochs=EPOCHS,\n","          validation_data=([query_pad_valid, value_pad_valid], value_shifted_pad_valid)\n","                   )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tHC9wxYtua0z","executionInfo":{"status":"aborted","timestamp":1702264043410,"user_tz":300,"elapsed":10,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"outputs":[],"source":["plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'], label='loss')\n","plt.plot(history.history['val_loss'], label='val_loss')\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch #')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['masked_acc'], label='accuracy')\n","plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch #')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","fr_en_history = history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oy0BbHdtuba9","executionInfo":{"status":"aborted","timestamp":1702264043410,"user_tz":300,"elapsed":9,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"outputs":[],"source":["# Save the model in the native Keras format\n","fr_en_model.save('fr_en_translation_model.keras')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPUM83Qlubvp"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4I0VXkijucBh"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ZttmOxTzDJW","executionInfo":{"status":"aborted","timestamp":1702264043410,"user_tz":300,"elapsed":9,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"outputs":[],"source":["def pad_seq(seq, max_length=50, pad_token_id=0):\n","        if len(seq) > max_length:\n","            pad = seq[:max_length]\n","        else:\n","            pad = seq + [pad_token_id]*(max_length-len(seq))\n","\n","        return pad"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZzbQArPJzDHj","executionInfo":{"status":"aborted","timestamp":1702264043410,"user_tz":300,"elapsed":9,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"outputs":[],"source":["def fr_en_predict(text_fr, model=fr_en_model, return_sent=False, pad_sent=False, print_gen=True):\n","    if pad_sent:\n","        sent_id_fr = text_fr\n","    else:\n","        clean_text_fr = clean_text(text_fr) #we clean the french sentence\n","        sent_token_fr, sent_id_fr = tokenize_fr(clean_text_fr) #convert the cleaned sentence into tokens of words and ids\n","        sent_id_fr = pad_seq(sent_id_fr[0], max_len) #we pad the sentence\n","    sent_id_en = np.zeros(max_len, dtype=np.int32) #the input english sentence is an array of 0 of max_len length\n","    sent_id_en[0] = tokenize_en.word_to_idx[start_token] #we replace the first token by the start_token because that is how our model learned to map the data\n","    predict = -1\n","    seq_en = []\n","    i = 0\n","    while predict!=2 and i<max_len-1: # 2 is the id of end_token in our dictionary\n","        softmax = tf.keras.layers.Softmax() #the output of the models are logits so we need to use a softmax layer\n","        test_predict = softmax(model.predict([np.expand_dims(sent_id_fr, axis=0),  np.expand_dims(sent_id_en, axis=0)], verbose=False)) #we produce the sequences output\n","        predict = np.argmax(test_predict[0][i]) #we only care about the token predicted for the ith token and select the one with the most probability\n","        seq_en += [predict]\n","        sent_id_en[i+1] = predict\n","        i +=1\n","    if print_gen:\n","        print(' '.join([tokenize_en.idx_to_word[i] for i in seq_en if i!=2])) #we print the generated sentence without the end_token\n","    if return_sent:\n","        return sent_id_en"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5dNM6YSGMeZW","executionInfo":{"status":"aborted","timestamp":1702264043410,"user_tz":300,"elapsed":8,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"outputs":[],"source":["def en_fr_predict(text_en, model=en_fr_model, return_sent=False, pad_sent=False, print_gen=True):\n","    if pad_sent:\n","        sent_id_en = text_en\n","    else:\n","        clean_text_en = clean_text(text_en) #we clean the french sentence\n","        sent_token_en, sent_id_en = tokenize_en(clean_text_en) #convert the cleaned sentence into tokens of words and ids\n","        sent_id_en = pad_seq(sent_id_en[0], max_len) #we pad the sentence\n","    sent_id_fr = np.zeros(max_len, dtype=np.int32) #the input english sentence is an array of 0 of max_len length\n","    sent_id_fr[0] = tokenize_fr.word_to_idx[start_token] #we replace the first token by the start_token because that is how our model learned to map the data\n","    predict = -1\n","    seq_fr = []\n","    i = 0\n","    while predict!=2 and i<max_len-1: # 2 is the id of end_token in our dictionary\n","        softmax = tf.keras.layers.Softmax() #the output of the models are logits so we need to use a softmax layer\n","        test_predict = softmax(model.predict([np.expand_dims(sent_id_en, axis=0),  np.expand_dims(sent_id_fr, axis=0)], verbose=False)) #we produce the sequences output\n","        predict = np.argmax(test_predict[0][i]) #we only care about the token predicted for the ith token and select the one with the most probability\n","        seq_fr += [predict]\n","        sent_id_fr[i+1] = predict\n","        i +=1\n","    if print_gen:\n","        print(' '.join([tokenize_fr.idx_to_word[i] for i in seq_fr if i!=2])) #we print the generated sentence without the end_token\n","    if return_sent:\n","        return sent_id_fr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3FIFoUcHzDBF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WkvchUpeppPa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702103027250,"user_tz":300,"elapsed":23066,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}},"outputId":"9a23fa9e-10de-4139-f9d3-5ba05b812366"},"outputs":[{"name":"stdout","output_type":"stream","text":["hi\n"]}],"source":[]},{"cell_type":"code","source":["text = \"hi\"\n","en_fr_predict(text, model= en_fr_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hCRjKHazGeyJ","executionInfo":{"status":"ok","timestamp":1702103035516,"user_tz":300,"elapsed":770,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}},"outputId":"a0e08760-c56f-45e9-c299-de93595f6e94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["< START > me dire il pas < END >\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1pt7C2UzC-z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702103052339,"user_tz":300,"elapsed":4035,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}},"outputId":"2fc6e083-a512-4a74-8c6f-919741610e77"},"outputs":[{"output_type":"stream","name":"stdout","text":["greater greater greater slacks demonstrated demonstrated moscow hooky rewritten abandon outskirts reluctant reluctant blondes rendered unsure blondes demonstrated shaping shaping outskirts shaping reluctant brand-name abandon abandon reluctant blondes dishcloth rendered unsure demonstrated shaping shaping transfixed abandon abandon shaping reluctant reluctant blondes dishcloth demonstrated abandon shaping governs demonstrated governs governs rendered demonstrated vices shaping shaping transfixed reluctant reluctant abandon reluctant blondes dishcloth rendered unsure\n"]}],"source":["text = \"me dire il pas\"\n","fr_en_predict(text, model=fr_en_model)"]},{"cell_type":"code","source":["from tensorflow.keras.layers import AdditiveAttention, Attention\n","\n","attention_layer = Attention(use_scale=True)  # You can use either AdditiveAttention or Attention layer.\n"],"metadata":{"id":"x-L20ysq4-XE","executionInfo":{"status":"aborted","timestamp":1702264043410,"user_tz":300,"elapsed":8,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define an input sequence and process it.\n","encoder_inputs1 = Input(shape=(max_len))\n","y = Embedding(fr_vocab, UNITS)(encoder_inputs1)\n","encoder_outputs1, state_h1, state_c1 = LSTM(UNITS, return_sequences=True, return_state=True, dropout=0.2)(y)\n","encoder_states1 = [state_h1, state_c1]\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs1 = Input(shape=(max_len))\n","decoder_embeddings1 = Embedding(en_vocab, UNITS)(decoder_inputs1)\n","\n","# Apply attention to the decoder inputs and encoder outputs\n","attention_output = attention_layer([decoder_embeddings1, encoder_outputs1])\n","\n","# Concatenate attention output and decoder inputs\n","decoder_input_attention = Concatenate(axis=-1)([decoder_embeddings1, attention_output])\n","\n","y = LSTM(UNITS, return_sequences=True, dropout=0.2)(decoder_input_attention, initial_state=encoder_states1)\n","decoder_outputs1 = Dense(en_vocab)(y)\n","\n","attention_model = Model([encoder_inputs1, decoder_inputs1], decoder_outputs1)\n","attention_model.summary()\n"],"metadata":{"id":"aecOEGsv4-Zq","executionInfo":{"status":"aborted","timestamp":1702264043411,"user_tz":300,"elapsed":8,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def masked_loss(y_true, y_pred):\n","    # Calculate the loss for each item in the batch.\n","    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True, reduction='none')\n","    loss = loss_fn(y_true, y_pred)\n","\n","    # Mask off the losses on padding.\n","    mask = tf.cast(y_true != 0, loss.dtype)\n","    loss *= mask\n","\n","    # Return the total.\n","    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n","\n","def masked_acc(y_true, y_pred):\n","    # Calculate the accuracy for each item in the batch.\n","    y_pred = tf.argmax(y_pred, axis=-1)\n","    y_pred = tf.cast(y_pred, y_true.dtype)\n","\n","    match = tf.cast(y_true == y_pred, tf.float32)\n","    mask = tf.cast(y_true != 0, tf.float32)\n","\n","    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n","attention_model.compile(optimizer='adam', loss=masked_loss, metrics=[masked_acc, masked_loss])\n","\n","# Train the attention model (replace the input data and parameters as needed)\n","history_attention = attention_model.fit([query_pad_train, value_pad_train], value_shifted_pad_train,\n","                                        batch_size=BATCH_SIZE,\n","                                        epochs=EPOCHS,\n","                                        validation_data=([query_pad_valid, value_pad_valid], value_shifted_pad_valid))"],"metadata":{"id":"ieVo5Kof4-cm","executionInfo":{"status":"aborted","timestamp":1702264043411,"user_tz":300,"elapsed":8,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.subplot(1, 2, 1)\n","plt.plot(history_attention.history['loss'], label='loss')\n","plt.plot(history_attention.history['val_loss'], label='val_loss')\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch #')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history_attention.history['masked_acc'], label='accuracy')\n","plt.plot(history_attention.history['val_masked_acc'], label='val_accuracy')\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch #')\n","plt.ylabel('Accuracy')\n","plt.legend()\n"],"metadata":{"id":"50tdqZoI4-fV","executionInfo":{"status":"aborted","timestamp":1702264043411,"user_tz":300,"elapsed":8,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def fr_en_predict_attention(text_fr, model=attention_model, return_sent=False, pad_sent=False, print_gen=True):\n","    if pad_sent:\n","        sent_id_fr = text_fr\n","    else:\n","        clean_text_fr = clean_text(text_fr)  # Clean the French sentence\n","        sent_token_fr, sent_id_fr = tokenize_fr(clean_text_fr)  # Convert the cleaned sentence into tokens of words and ids\n","        sent_id_fr = pad_seq(sent_id_fr[0], max_len)  # Pad the sentence\n","\n","    sent_id_en = np.zeros(max_len, dtype=np.int32)  # The input English sentence is an array of 0 of max_len length\n","    sent_id_en[0] = tokenize_en.word_to_idx[start_token]  # Replace the first token by the start_token because that is how our model learned to map the data\n","\n","    predict = -1\n","    seq_en = []\n","    i = 0\n","    while predict != 2 and i < max_len - 1:  # 2 is the id of end_token in our dictionary\n","        softmax = tf.keras.layers.Softmax()  # The output of the models are logits so we need to use a softmax layer\n","        test_predict = softmax(model.predict([np.expand_dims(sent_id_fr, axis=0), np.expand_dims(sent_id_en, axis=0)], verbose=False))  # We produce the sequences output\n","        predict = np.argmax(test_predict[0][i])  # We only care about the token predicted for the ith token and select the one with the most probability\n","        seq_en += [predict]\n","        sent_id_en[i + 1] = predict\n","        i += 1\n","\n","    if print_gen:\n","        print(' '.join([tokenize_en.idx_to_word[i] for i in seq_en if i != 2]))  # We print the generated sentence without the end_token\n","    if return_sent:\n","        return sent_id_en\n"],"metadata":{"id":"VoA6STdTQFQ_","executionInfo":{"status":"aborted","timestamp":1702264043411,"user_tz":300,"elapsed":8,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"cours\"\n","fr_en_predict(text, model=attention_model)"],"metadata":{"id":"S4YhzOpCQFYr","executionInfo":{"status":"aborted","timestamp":1702264043411,"user_tz":300,"elapsed":8,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attention_model.save('fr_en_attention_model_translation_model.keras')"],"metadata":{"id":"dZWNvycEQFdn","executionInfo":{"status":"aborted","timestamp":1702264043411,"user_tz":300,"elapsed":8,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4KTEv_d5QFg2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define an input sequence and process it.\n","encoder_inputs1 = Input(shape=(max_len))\n","y = Embedding(en_vocab, UNITS)(encoder_inputs1)\n","encoder_outputs1, state_h1, state_c1 = LSTM(UNITS, return_sequences=True, return_state=True, dropout=0.2)(y)\n","encoder_states1 = [state_h1, state_c1]\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs1 = Input(shape=(max_len))\n","decoder_embeddings1 = Embedding(fr_vocab, UNITS)(decoder_inputs1)\n","\n","# Apply attention to the decoder inputs and encoder outputs\n","attention_output = attention_layer([decoder_embeddings1, encoder_outputs1])\n","\n","# Concatenate attention output and decoder inputs\n","decoder_input_attention = Concatenate(axis=-1)([decoder_embeddings1, attention_output])\n","\n","y = LSTM(UNITS, return_sequences=True, dropout=0.2)(decoder_input_attention, initial_state=encoder_states1)\n","decoder_outputs1 = Dense(fr_vocab)(y)\n","\n","attention_model2 = Model([encoder_inputs1, decoder_inputs1], decoder_outputs1)\n","attention_model2.summary()"],"metadata":{"id":"R_2YF_qa4-h_","executionInfo":{"status":"aborted","timestamp":1702264043411,"user_tz":300,"elapsed":7,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attention_model2.compile(optimizer='adam', loss=masked_loss, metrics=[masked_acc, masked_loss])\n","\n","# Train the attention model (replace the input data and parameters as needed)\n","history_attention2 = attention_model2.fit([query_pad_train, value_pad_train], value_shifted_pad_train,\n","                                        batch_size=BATCH_SIZE,\n","                                        epochs=EPOCHS,\n","                                        validation_data=([query_pad_valid, value_pad_valid], value_shifted_pad_valid))"],"metadata":{"id":"01oFx8Eq4-ko","executionInfo":{"status":"aborted","timestamp":1702264043411,"user_tz":300,"elapsed":7,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.subplot(1, 2, 1)\n","plt.plot(history_attention2.history['loss'], label='loss')\n","plt.plot(history_attention2.history['val_loss'], label='val_loss')\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch #')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history_attention2.history['masked_acc'], label='accuracy')\n","plt.plot(history_attention2.history['val_masked_acc'], label='val_accuracy')\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch #')\n","plt.ylabel('Accuracy')\n","plt.legend()"],"metadata":{"id":"aeypP_pISfjp","executionInfo":{"status":"aborted","timestamp":1702264043411,"user_tz":300,"elapsed":7,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def en_fr_predict_attention(text_en, model=attention_model2, return_sent=False, pad_sent=False, print_gen=True):\n","    if pad_sent:\n","        sent_id_en = text_en\n","    else:\n","        clean_text_en = clean_text(text_en)  # Clean the French sentence\n","        sent_token_en, sent_id_en = tokenize_en(clean_text_en)  # Convert the cleaned sentence into tokens of words and ids\n","        sent_id_en = pad_seq(sent_id_en[0], max_len)  # Pad the sentence\n","\n","    sent_id_fr = np.zeros(max_len, dtype=np.int32)  # The input English sentence is an array of 0 of max_len length\n","    sent_id_fr[0] = tokenize_fr.word_to_idx[start_token]  # Replace the first token by the start_token because that is how our model learned to map the data\n","\n","    predict = -1\n","    seq_fr = []\n","    i = 0\n","    while predict != 2 and i < max_len - 1:  # 2 is the id of end_token in our dictionary\n","        softmax = tf.keras.layers.Softmax()  # The output of the models are logits so we need to use a softmax layer\n","        test_predict = softmax(model.predict([np.expand_dims(sent_id_en, axis=0), np.expand_dims(sent_id_fr, axis=0)], verbose=False))  # We produce the sequences output\n","        predict = np.argmax(test_predict[0][i])  # We only care about the token predicted for the ith token and select the one with the most probability\n","        seq_fr += [predict]\n","        sent_id_fr[i + 1] = predict\n","        i += 1\n","\n","    if print_gen:\n","        print(' '.join([tokenize_en.idx_to_word[i] for i in seq_fr if i != 2]))  # We print the generated sentence without the end_token\n","    if return_sent:\n","        return sent_id_fr"],"metadata":{"id":"PcY4XW4fS819","executionInfo":{"status":"aborted","timestamp":1702264043412,"user_tz":300,"elapsed":8,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attention_model2.save('en_fr_attention_model_translation_model.keras')"],"metadata":{"id":"KCiOknHy4-n_","executionInfo":{"status":"aborted","timestamp":1702264043412,"user_tz":300,"elapsed":8,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"En8Z0y1VzC8m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702106765038,"user_tz":300,"elapsed":969,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}},"outputId":"0d181ed6-2c53-4d45-80f4-1f6395ba9988"},"outputs":[{"output_type":"stream","name":"stdout","text":["< START > a suis en quelque debarrasser . < END >\n"]}],"source":["text = \"? stay my you do.\"\n","en_fr_predict(text, model=attention_model2)"]},{"cell_type":"markdown","metadata":{"id":"sw3Is-9MYPse"},"source":["#Predictions"]},{"cell_type":"code","source":[],"metadata":{"id":"spaD6EunjYNR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"G4Y3KtnDjYPn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eng_text = \"Hi, How are you?\"\n","fre_text = \"Salut comment vas-tu?\""],"metadata":{"id":"YpMWOVPehWwO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IcyeSuXQhWzT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fr_en_predict(fre_text, model=fr_en_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EwYTEwtVhW18","executionInfo":{"status":"ok","timestamp":1702176095542,"user_tz":300,"elapsed":941,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}},"outputId":"d1e23c84-bdf5-4706-b23e-edebe18c1a4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["< START > is it nearby or you ? < END >\n"]}]},{"cell_type":"code","source":["en_fr_predict(eng_text, model=en_fr_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GR5g4r3BhW4c","executionInfo":{"status":"ok","timestamp":1702176096671,"user_tz":300,"elapsed":1136,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}},"outputId":"0cdba3f5-0576-4387-e0e2-2536640d02b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["< START > je c'est plus deux vous a ! part . < END >\n"]}]},{"cell_type":"code","source":["fr_en_predict_attention(fre_text, model=attention_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7U1KyM4hW7C","executionInfo":{"status":"ok","timestamp":1702176097439,"user_tz":300,"elapsed":773,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}},"outputId":"396ddd1a-a2bb-4535-e1e3-32cfb39c3f59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["< START > hi , how are you ? < END >\n"]}]},{"cell_type":"code","source":["en_fr_predict_attention(eng_text, model=attention_model2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdRv9H8XhW9Y","executionInfo":{"status":"ok","timestamp":1702176209622,"user_tz":300,"elapsed":1431,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}},"outputId":"7bd052ae-38b9-45eb-d103-b1e9c6e87aa8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["< START > what it is to be not for me ? < END >\n"]}]},{"cell_type":"code","source":["from langdetect import detect\n","\n","def detect_language(text):\n","    try:\n","        language = detect(text)\n","        return language\n","    except:\n","        return \"Language detection failed\"\n","\n","# Input text for language detection\n","text = input(\"Enter the text: \")\n","\n","# Detect the language of the input text\n","language = detect_language(text)\n","\n","# Print the detected language\n","print(\"Detected Language:\", language)"],"metadata":{"id":"n2KheJRtqqS7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import langdetect\n","def identify_language(text):\n","    try:\n","        return detect(text)\n","    except:\n","        return \"Language detection failed\"\n","\n","text = input(\"Enter the text: \")\n","model = input(\"Enter the model (LSTM or LSTM with attention): \")\n","\n","if identify_language(text) == \"en\" and model == \"LSTM\":\n","    translation = en_fr_predict(text, model=en_fr_model)\n","elif identify_language(text) == \"fr\" and model == \"LSTM\":\n","    translation = fr_en_predict(text, model=fr_en_model)\n","elif identify_language(text) == \"en\" and model == \"LSTM with attention\":\n","    translation = en_fr_predict_attention(text, model=attention_model)\n","elif identify_language(text) == \"fr\" and model == \"LSTM with attention\":\n","    translation = fr_en_predict_attention(text, model=attention_model2)\n","else:\n","    translation = \"Unsupported language or model type\"\n","print(identify_language(text))\n","print(\"Translation:\", translation)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WB9r7xOaiky0","executionInfo":{"status":"ok","timestamp":1702177070269,"user_tz":300,"elapsed":4853,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}},"outputId":"59ce26fa-3cad-4c2c-e228-b5e47d45256a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter the text: Hi\n","Enter the model (LSTM or LSTM with attention): LSTM\n","Language detection failed\n","Translation: Unsupported language or model type\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"K_b6nexKnq2O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dO8TeGGioP74"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Q6naK_Ffnrma","executionInfo":{"status":"ok","timestamp":1702177131639,"user_tz":300,"elapsed":225,"user":{"displayName":"Naveen Venkat","userId":"13505772317741884313"}},"outputId":"6e53ebe6-8810-48da-a60c-016b12e2f5df"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'fi'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":[],"metadata":{"id":"t4TBM-xSqYWQ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"15LqQA12UdCepgrBHZu7pU0gVp5QhQpDs","timestamp":1701716294005}],"machine_shape":"hm","gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}